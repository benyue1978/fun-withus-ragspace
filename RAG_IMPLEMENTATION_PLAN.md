# RAGSpace RAG Implementation Plan

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†RAGSpaceé¡¹ç›®ä¸­RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„å®ç°è®¡åˆ’ï¼ŒåŒ…æ‹¬å…·ä½“çš„å®ç°æ­¥éª¤ã€ä»£ç ç»“æ„ã€æ—¶é—´å®‰æ’å’ŒéªŒæ”¶æ ‡å‡†ã€‚

## å®ç°é˜¶æ®µ

### Phase 1: æ ¸å¿ƒRAGåŸºç¡€è®¾æ–½ (Week 1)

#### 1.1 æ•°æ®åº“æ¶æ„å®ç°

**ç›®æ ‡**: å»ºç«‹å‘é‡æ•°æ®åº“åŸºç¡€è®¾æ–½

**ä»»åŠ¡**:
- [ ] åˆ›å»ºchunksè¡¨
- [ ] é…ç½®pgvectoræ‰©å±•
- [ ] å»ºç«‹å‘é‡ç´¢å¼•
- [ ] æ·»åŠ æ–‡æ¡£çŠ¶æ€ç®¡ç†

**å®ç°æ­¥éª¤**:

1. **åˆ›å»ºchunksè¡¨**
```sql
-- åœ¨Supabase SQL Editorä¸­æ‰§è¡Œ
CREATE TABLE chunks (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  docset_name TEXT NOT NULL,
  document_name TEXT NOT NULL,
  chunk_index INTEGER NOT NULL,
  content TEXT NOT NULL,
  embedding VECTOR(1536),
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP DEFAULT NOW()
);

-- åˆ›å»ºå‘é‡ç´¢å¼•
CREATE INDEX ON chunks USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX idx_chunks_docset_document ON chunks(docset_name, document_name);
CREATE INDEX idx_chunks_metadata ON chunks USING GIN (metadata);
```

2. **æ›´æ–°documentsè¡¨**
```sql
-- æ·»åŠ åµŒå…¥çŠ¶æ€å­—æ®µ
ALTER TABLE documents ADD COLUMN IF NOT EXISTS embedding_status TEXT 
DEFAULT 'pending' CHECK (embedding_status IN ('pending', 'processing', 'done', 'error'));

-- æ·»åŠ åµŒå…¥æ—¶é—´å­—æ®µ
ALTER TABLE documents ADD COLUMN IF NOT EXISTS embedding_updated_at TIMESTAMP;
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] chunksè¡¨æˆåŠŸåˆ›å»º
- [ ] å‘é‡ç´¢å¼•æ­£å¸¸å·¥ä½œ
- [ ] æ–‡æ¡£çŠ¶æ€å­—æ®µæ­£ç¡®æ·»åŠ 
- [ ] æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½è‰¯å¥½

#### 1.2 æ–‡æœ¬åˆ†ç‰‡å™¨å®ç°

**ç›®æ ‡**: å®ç°æ™ºèƒ½æ–‡æ¡£åˆ†ç‰‡åŠŸèƒ½

**ä»»åŠ¡**:
- [ ] å®ç°åŸºç¡€æ–‡æœ¬åˆ†ç‰‡å™¨
- [ ] å®ç°ä»£ç åˆ†ç‰‡å™¨
- [ ] æ·»åŠ åˆ†ç‰‡ç­–ç•¥é€‰æ‹©
- [ ] é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ

**å®ç°æ­¥éª¤**:

1. **åˆ›å»ºæ–‡æœ¬åˆ†ç‰‡å™¨æ¨¡å—**
```python
# src/ragspace/rag/text_splitter.py
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict

class RAGTextSplitter:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            separators=["\n\n", "\n", ".", " "],
            length_function=len
        )
        
        self.code_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=50,
            separators=["\nclass ", "\ndef ", "\n", " "],
            length_function=len
        )
    
    def split_text(self, content: str, doc_type: str = "text") -> List[Dict]:
        """åˆ†ç‰‡æ–‡æœ¬å†…å®¹"""
        if doc_type in ['github_file', 'code']:
            chunks = self.code_splitter.split_text(content)
        else:
            chunks = self.text_splitter.split_text(content)
        
        return [
            {
                "content": chunk.page_content,
                "metadata": {
                    "chunk_index": i,
                    "doc_type": doc_type,
                    "length": len(chunk.page_content)
                }
            }
            for i, chunk in enumerate(chunks)
        ]
```

2. **é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ**
```python
# src/ragspace/services/__init__.py
from .rag.text_splitter import RAGTextSplitter

# åœ¨ç°æœ‰æœåŠ¡ä¸­ä½¿ç”¨
text_splitter = RAGTextSplitter()
chunks = text_splitter.split_text(document_content, doc_type)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ–‡æœ¬åˆ†ç‰‡å™¨æ­£å¸¸å·¥ä½œ
- [ ] ä»£ç åˆ†ç‰‡å™¨æ­£ç¡®å¤„ç†ä»£ç æ–‡ä»¶
- [ ] åˆ†ç‰‡å¤§å°å’Œé‡å è®¾ç½®åˆç†
- [ ] ä¸ç°æœ‰ç³»ç»Ÿé›†æˆè‰¯å¥½

#### 1.3 åµŒå…¥å·¥ä½œå™¨å®ç°

**ç›®æ ‡**: å®ç°å¼‚æ­¥æ–‡æ¡£åµŒå…¥å¤„ç†

**ä»»åŠ¡**:
- [ ] å®ç°åµŒå…¥å·¥ä½œå™¨ç±»
- [ ] æ·»åŠ å¼‚æ­¥å¤„ç†æ”¯æŒ
- [ ] å®ç°çŠ¶æ€ç®¡ç†
- [ ] æ·»åŠ é”™è¯¯å¤„ç†

**å®ç°æ­¥éª¤**:

1. **åˆ›å»ºåµŒå…¥å·¥ä½œå™¨**
```python
# src/ragspace/rag/embedding_worker.py
import asyncio
import logging
from typing import List, Dict
from supabase import create_client
import openai

logger = logging.getLogger(__name__)

class EmbeddingWorker:
    def __init__(self, model_name="openai"):
        self.model_name = model_name
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_ANON_KEY')
        )
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY')
        )
    
    async def process_document(self, doc_id: str):
        """å¤„ç†å•ä¸ªæ–‡æ¡£çš„åµŒå…¥"""
        try:
            # 1. æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self._update_status(doc_id, "processing")
            
            # 2. è·å–æ–‡æ¡£å†…å®¹
            document = self._get_document(doc_id)
            
            # 3. åˆ†ç‰‡æ–‡æ¡£
            text_splitter = RAGTextSplitter()
            chunks = text_splitter.split_text(
                document['content'], 
                document.get('doc_type', 'text')
            )
            
            # 4. ç”ŸæˆåµŒå…¥
            embeddings = await self._generate_embeddings(chunks)
            
            # 5. å­˜å‚¨åˆ°æ•°æ®åº“
            self._store_chunks(document, chunks, embeddings)
            
            # 6. æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            self._update_status(doc_id, "done")
            
        except Exception as e:
            self._update_status(doc_id, "error")
            logger.error(f"Embedding failed for doc {doc_id}: {e}")
    
    async def _generate_embeddings(self, chunks: List[Dict]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        texts = [chunk['content'] for chunk in chunks]
        
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        
        return [embedding.embedding for embedding in response.data]
    
    def _store_chunks(self, document: Dict, chunks: List[Dict], embeddings: List[List[float]]):
        """å­˜å‚¨åˆ†ç‰‡å’ŒåµŒå…¥åˆ°æ•°æ®åº“"""
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_data = {
                "docset_name": document['docset_name'],
                "document_name": document['title'],
                "chunk_index": i,
                "content": chunk['content'],
                "embedding": embedding,
                "metadata": {
                    **chunk['metadata'],
                    "source_type": document.get('source_type', 'file'),
                    "url": document.get('url', ''),
                    "file_path": document.get('file_path', ''),
                    "language": document.get('language', ''),
                    "repo": document.get('repo', ''),
                    "commit_id": document.get('commit_id', '')
                }
            }
            
            self.supabase.table("chunks").insert(chunk_data).execute()
    
    def _update_status(self, doc_id: str, status: str):
        """æ›´æ–°æ–‡æ¡£åµŒå…¥çŠ¶æ€"""
        self.supabase.table("documents").update({
            "embedding_status": status,
            "embedding_updated_at": datetime.now().isoformat()
        }).eq("id", doc_id).execute()
    
    def _get_document(self, doc_id: str) -> Dict:
        """è·å–æ–‡æ¡£å†…å®¹"""
        response = self.supabase.table("documents").select("*").eq("id", doc_id).execute()
        return response.data[0] if response.data else None
```

2. **æ·»åŠ æ‰¹é‡å¤„ç†åŠŸèƒ½**
```python
async def batch_process(self, docset_name: str = None):
    """æ‰¹é‡å¤„ç†å¾…åµŒå…¥çš„æ–‡æ¡£"""
    query = self.supabase.table("documents").select("id").eq("embedding_status", "pending")
    
    if docset_name:
        query = query.eq("docset_name", docset_name)
    
    response = query.execute()
    pending_docs = response.data
    
    for doc in pending_docs:
        await self.process_document(doc['id'])
        await asyncio.sleep(1)  # é¿å…APIé™åˆ¶
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] åµŒå…¥å·¥ä½œå™¨æ­£å¸¸å·¥ä½œ
- [ ] å¼‚æ­¥å¤„ç†æ”¯æŒè‰¯å¥½
- [ ] çŠ¶æ€ç®¡ç†æ­£ç¡®
- [ ] é”™è¯¯å¤„ç†å®Œå–„

### Phase 2: æ£€ç´¢ç³»ç»Ÿå®ç° (Week 2)

#### 2.1 å‘é‡æ£€ç´¢å®ç°

**ç›®æ ‡**: å®ç°é«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æœç´¢

**ä»»åŠ¡**:
- [ ] å®ç°å‘é‡æ£€ç´¢åŠŸèƒ½
- [ ] æ·»åŠ DocSetè¿‡æ»¤
- [ ] ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
- [ ] æ·»åŠ ç»“æœç¼“å­˜

**å®ç°æ­¥éª¤**:

1. **åˆ›å»ºæ£€ç´¢å™¨ç±»**
```python
# src/ragspace/rag/retriever.py
import numpy as np
from typing import List, Dict, Optional
from supabase import create_client
import openai

class RAGRetriever:
    def __init__(self):
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_ANON_KEY')
        )
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY')
        )
    
    def retrieve_chunks(self, query: str, docsets: List[str] = None, top_k: int = 5) -> List[Dict]:
        """å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢"""
        # 1. ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self._generate_query_embedding(query)
        
        # 2. æ„å»ºæŸ¥è¯¢
        query_builder = self.supabase.table("chunks").select("*")
        
        if docsets and "all" not in docsets:
            query_builder = query_builder.in_("docset_name", docsets)
        
        # 3. å‘é‡ç›¸ä¼¼åº¦æœç´¢
        results = query_builder.order(
            f"embedding <-> '{query_embedding}'"
        ).limit(top_k).execute()
        
        return results.data
    
    def _generate_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢åµŒå…¥"""
        response = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        return response.data[0].embedding
    
    def hybrid_retrieve(self, query: str, docsets: List[str] = None, 
                       top_k: int = 5, use_rerank: bool = True) -> List[Dict]:
        """æ··åˆæ£€ç´¢ç­–ç•¥"""
        # 1. å‘é‡æ£€ç´¢è·å–å€™é€‰
        candidates = self.retrieve_chunks(query, docsets, top_k * 2)
        
        if not use_rerank:
            return candidates[:top_k]
        
        # 2. GPTé‡æ’åº
        return self._gpt_rerank(query, candidates, top_k)
    
    def _gpt_rerank(self, query: str, chunks: List[Dict], top_k: int = 3) -> List[Dict]:
        """ä½¿ç”¨GPTè¿›è¡Œé‡æ’åº"""
        if not chunks:
            return []
        
        # æ„å»ºé‡æ’åºæç¤º
        prompt = self._build_rerank_prompt(query, chunks)
        
        response = self.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        
        try:
            ranking = json.loads(response.choices[0].message.content)
            return [chunks[i] for i in ranking[:top_k] if i < len(chunks)]
        except (json.JSONDecodeError, IndexError):
            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ
            return chunks[:top_k]
    
    def _build_rerank_prompt(self, query: str, chunks: List[Dict]) -> str:
        """æ„å»ºé‡æ’åºæç¤º"""
        snippets_text = "\n".join([
            f"{i+1}. {chunk['content'][:200]}..." 
            for i, chunk in enumerate(chunks)
        ])
        
        return f"""
        You are an expert technical assistant.
        
        Given the following user question and a list of code/document snippets retrieved from a knowledge base, rank the snippets by how relevant they are to answering the question.
        
        Question: {query}
        
        Snippets:
        {snippets_text}
        
        Return the ranking as a JSON list of indices sorted from most to least relevant. Do not explain.
        Example: [2, 1, 3]
        """
```

2. **æ·»åŠ ç¼“å­˜æœºåˆ¶**
```python
import redis
import hashlib
import json

class CachedRetriever(RAGRetriever):
    def __init__(self):
        super().__init__()
        self.redis_client = redis.Redis(
            host=os.getenv('REDIS_HOST', 'localhost'),
            port=int(os.getenv('REDIS_PORT', 6379)),
            db=0
        )
    
    def retrieve_chunks(self, query: str, docsets: List[str] = None, top_k: int = 5) -> List[Dict]:
        """å¸¦ç¼“å­˜çš„å‘é‡æ£€ç´¢"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(query, docsets, top_k)
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self.redis_client.get(cache_key)
        if cached_result:
            return json.loads(cached_result)
        
        # æ‰§è¡Œæ£€ç´¢
        result = super().retrieve_chunks(query, docsets, top_k)
        
        # ç¼“å­˜ç»“æœï¼ˆ5åˆ†é’Ÿè¿‡æœŸï¼‰
        self.redis_client.setex(cache_key, 300, json.dumps(result))
        
        return result
    
    def _generate_cache_key(self, query: str, docsets: List[str], top_k: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {
            "query": query,
            "docsets": sorted(docsets or []),
            "top_k": top_k
        }
        key_string = json.dumps(key_data, sort_keys=True)
        return f"rag_retrieval:{hashlib.md5(key_string.encode()).hexdigest()}"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] å‘é‡æ£€ç´¢åŠŸèƒ½æ­£å¸¸
- [ ] DocSetè¿‡æ»¤æ­£ç¡®å·¥ä½œ
- [ ] æŸ¥è¯¢æ€§èƒ½è‰¯å¥½ï¼ˆ< 1ç§’ï¼‰
- [ ] ç¼“å­˜æœºåˆ¶æœ‰æ•ˆ

#### 2.2 ä¸Šä¸‹æ–‡ç»„è£…å®ç°

**ç›®æ ‡**: å®ç°æ™ºèƒ½ä¸Šä¸‹æ–‡ç»„è£…åŠŸèƒ½

**ä»»åŠ¡**:
- [ ] å®ç°ä¸Šä¸‹æ–‡ç»„è£…å™¨
- [ ] æ·»åŠ æºä¿¡æ¯ç®¡ç†
- [ ] ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦
- [ ] æ·»åŠ ä¸Šä¸‹æ–‡è´¨é‡è¯„ä¼°

**å®ç°æ­¥éª¤**:

1. **åˆ›å»ºä¸Šä¸‹æ–‡ç»„è£…å™¨**
```python
# src/ragspace/rag/context_assembler.py
from typing import List, Dict
import re

class ContextAssembler:
    def __init__(self, max_context_length: int = 4000):
        self.max_context_length = max_context_length
    
    def assemble_context(self, chunks: List[Dict]) -> str:
        """ç»„è£…ä¸Šä¸‹æ–‡"""
        context_parts = []
        current_length = 0
        
        for chunk in chunks:
            # æ„å»ºæºä¿¡æ¯
            source_info = self._build_source_info(chunk)
            
            # æ„å»ºå†…å®¹
            content = chunk['content']
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            part_length = len(source_info) + len(content) + 10  # é¢å¤–ç©ºé—´
            if current_length + part_length > self.max_context_length:
                break
            
            # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            context_parts.append(f"{source_info}\n{content}\n")
            current_length += part_length
        
        return "\n".join(context_parts)
    
    def _build_source_info(self, chunk: Dict) -> str:
        """æ„å»ºæºä¿¡æ¯"""
        metadata = chunk.get('metadata', {})
        
        source_parts = []
        
        # æ–‡æ¡£åç§°
        if 'document_name' in metadata:
            source_parts.append(f"Document: {metadata['document_name']}")
        
        # æ–‡ä»¶è·¯å¾„
        if 'file_path' in metadata:
            source_parts.append(f"File: {metadata['file_path']}")
        
        # è¡Œå·ä¿¡æ¯
        if 'start_line' in metadata and 'end_line' in metadata:
            source_parts.append(f"Lines: {metadata['start_line']}-{metadata['end_line']}")
        
        # ä»“åº“ä¿¡æ¯
        if 'repo' in metadata:
            source_parts.append(f"Repository: {metadata['repo']}")
        
        # è¯­è¨€ä¿¡æ¯
        if 'language' in metadata:
            source_parts.append(f"Language: {metadata['language']}")
        
        if source_parts:
            return f"Source: {' | '.join(source_parts)}"
        else:
            return "Source: Unknown"
    
    def evaluate_context_quality(self, chunks: List[Dict]) -> Dict:
        """è¯„ä¼°ä¸Šä¸‹æ–‡è´¨é‡"""
        if not chunks:
            return {"score": 0, "issues": ["No chunks provided"]}
        
        issues = []
        score = 100
        
        # æ£€æŸ¥å¤šæ ·æ€§
        unique_docs = len(set(chunk.get('metadata', {}).get('document_name', '') for chunk in chunks))
        if unique_docs < 2:
            issues.append("Limited document diversity")
            score -= 20
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        total_length = sum(len(chunk['content']) for chunk in chunks)
        if total_length < 500:
            issues.append("Context too short")
            score -= 15
        elif total_length > self.max_context_length:
            issues.append("Context too long")
            score -= 10
        
        # æ£€æŸ¥ç›¸å…³æ€§ï¼ˆåŸºäºchunké¡ºåºï¼‰
        if len(chunks) > 1:
            # ç®€å•çš„ç›¸å…³æ€§æ£€æŸ¥ï¼šç›¸é‚»chunkæ¥è‡ªåŒä¸€æ–‡æ¡£
            consecutive_same_doc = 0
            for i in range(len(chunks) - 1):
                doc1 = chunks[i].get('metadata', {}).get('document_name', '')
                doc2 = chunks[i+1].get('metadata', {}).get('document_name', '')
                if doc1 == doc2:
                    consecutive_same_doc += 1
            
            if consecutive_same_doc > len(chunks) * 0.7:
                issues.append("Low content diversity")
                score -= 15
        
        return {
            "score": max(0, score),
            "issues": issues,
            "total_chunks": len(chunks),
            "total_length": total_length,
            "unique_documents": unique_docs
        }
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ä¸Šä¸‹æ–‡ç»„è£…åŠŸèƒ½æ­£å¸¸
- [ ] æºä¿¡æ¯ç®¡ç†æ­£ç¡®
- [ ] é•¿åº¦é™åˆ¶æœ‰æ•ˆ
- [ ] è´¨é‡è¯„ä¼°å‡†ç¡®

### Phase 3: å“åº”ç”Ÿæˆç³»ç»Ÿ (Week 3)

#### 3.1 LLMå“åº”ç”Ÿæˆå®ç°

**ç›®æ ‡**: å®ç°åŸºäºä¸Šä¸‹æ–‡çš„LLMå“åº”ç”Ÿæˆ

**ä»»åŠ¡**:
- [ ] å®ç°LLMå“åº”ç”Ÿæˆå™¨
- [ ] æ·»åŠ æµå¼å“åº”æ”¯æŒ
- [ ] å®ç°å¯¹è¯å†å²ç®¡ç†
- [ ] æ·»åŠ å“åº”è´¨é‡è¯„ä¼°

**å®ç°æ­¥éª¤**:

1. **åˆ›å»ºLLMå“åº”ç”Ÿæˆå™¨**
```python
# src/ragspace/rag/response_generator.py
import openai
from typing import List, Dict, Generator
import json

class RAGResponseGenerator:
    def __init__(self, model: str = "gpt-3.5-turbo"):
        self.model = model
        self.openai_client = openai.OpenAI(
            api_key=os.getenv('OPENAI_API_KEY')
        )
    
    def generate_response(self, query: str, context: str, 
                         conversation_history: List[Dict] = None) -> str:
        """ç”Ÿæˆå“åº”"""
        messages = self._build_messages(query, context, conversation_history)
        
        response = self.openai_client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        return response.choices[0].message.content
    
    def generate_streaming_response(self, query: str, context: str,
                                  conversation_history: List[Dict] = None) -> Generator[str, None, None]:
        """ç”Ÿæˆæµå¼å“åº”"""
        messages = self._build_messages(query, context, conversation_history)
        
        response = self.openai_client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=1000,
            stream=True
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    def _build_messages(self, query: str, context: str, 
                       conversation_history: List[Dict] = None) -> List[Dict]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        system_prompt = self._get_system_prompt()
        messages = [{"role": "system", "content": system_prompt}]
        
        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            for msg in conversation_history[-6:]:  # ä¿ç•™æœ€è¿‘6æ¡æ¶ˆæ¯
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        # æ·»åŠ å½“å‰æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡
        user_prompt = self._build_user_prompt(query, context)
        messages.append({"role": "user", "content": user_prompt})
        
        return messages
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤º"""
        return """You are a helpful AI assistant with access to a knowledge base.

Your role is to:
1. Answer questions based on the provided context
2. Always cite your sources when possible
3. Be accurate and helpful
4. If the context doesn't contain enough information, say so
5. Provide clear and concise answers

When citing sources, use the format: [Document Name, Lines X-Y]"""
    
    def _build_user_prompt(self, query: str, context: str) -> str:
        """æ„å»ºç”¨æˆ·æç¤º"""
        return f"""Context:
{context}

Question: {query}

Please provide a comprehensive answer based on the context above. If you cite information from the context, please mention the source."""
    
    def evaluate_response_quality(self, response: str, query: str, context: str) -> Dict:
        """è¯„ä¼°å“åº”è´¨é‡"""
        issues = []
        score = 100
        
        # æ£€æŸ¥å“åº”é•¿åº¦
        if len(response) < 50:
            issues.append("Response too short")
            score -= 20
        elif len(response) > 2000:
            issues.append("Response too long")
            score -= 10
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æºå¼•ç”¨
        if "[" in response and "]" in response:
            score += 10
        else:
            issues.append("No source citations")
            score -= 15
        
        # æ£€æŸ¥æ˜¯å¦å›ç­”äº†é—®é¢˜
        if not any(word in response.lower() for word in query.lower().split()):
            issues.append("Response may not address the question")
            score -= 20
        
        # æ£€æŸ¥æ˜¯å¦æ‰¿è®¤ä¿¡æ¯ä¸è¶³
        if "don't have enough information" in response.lower() or "context doesn't contain" in response.lower():
            score += 5  # è¯šå®å›ç­”åŠ åˆ†
        
        return {
            "score": max(0, score),
            "issues": issues,
            "length": len(response),
            "has_citations": "[" in response and "]" in response,
            "addresses_question": any(word in response.lower() for word in query.lower().split())
        }
```

2. **æ·»åŠ å¯¹è¯å†å²ç®¡ç†**
```python
# src/ragspace/rag/conversation_manager.py
from typing import List, Dict
import json
from datetime import datetime

class ConversationManager:
    def __init__(self, max_history_length: int = 10):
        self.max_history_length = max_history_length
    
    def add_message(self, conversation_id: str, role: str, content: str, 
                   metadata: Dict = None) -> Dict:
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        supabase.table("messages").insert({
            "conversation_id": conversation_id,
            "role": role,
            "content": content,
            "metadata": metadata or {}
        }).execute()
        
        return message
    
    def get_conversation_history(self, conversation_id: str) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        response = supabase.table("messages").select("*").eq(
            "conversation_id", conversation_id
        ).order("created_at").execute()
        
        return response.data
    
    def create_conversation(self, user_id: str, title: str = None) -> str:
        """åˆ›å»ºæ–°å¯¹è¯"""
        response = supabase.table("conversations").insert({
            "user_id": user_id,
            "title": title or f"Conversation {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        }).execute()
        
        return response.data[0]["id"]
    
    def update_conversation_title(self, conversation_id: str, title: str):
        """æ›´æ–°å¯¹è¯æ ‡é¢˜"""
        supabase.table("conversations").update({
            "title": title
        }).eq("id", conversation_id).execute()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] LLMå“åº”ç”ŸæˆåŠŸèƒ½æ­£å¸¸
- [ ] æµå¼å“åº”æ”¯æŒè‰¯å¥½
- [ ] å¯¹è¯å†å²ç®¡ç†æ­£ç¡®
- [ ] å“åº”è´¨é‡è¯„ä¼°å‡†ç¡®

### Phase 4: UIé›†æˆ (Week 4)

#### 4.1 RAGèŠå¤©ç•Œé¢å¢å¼º

**ç›®æ ‡**: å°†RAGåŠŸèƒ½é›†æˆåˆ°ç°æœ‰èŠå¤©ç•Œé¢

**ä»»åŠ¡**:
- [ ] å¢å¼ºç°æœ‰èŠå¤©ç•Œé¢
- [ ] æ·»åŠ DocSeté€‰æ‹©å™¨
- [ ] å®ç°æ£€ç´¢ç»“æœæ˜¾ç¤º
- [ ] æ·»åŠ åµŒå…¥çŠ¶æ€æ˜¾ç¤º

**å®ç°æ­¥éª¤**:

1. **å¢å¼ºèŠå¤©ç•Œé¢ç»„ä»¶**
```python
# src/ragspace/ui/components/rag_chat.py
import gradio as gr
from typing import List, Dict
from ..rag.retriever import RAGRetriever
from ..rag.response_generator import RAGResponseGenerator
from ..rag.context_assembler import ContextAssembler

class RAGChatInterface:
    def __init__(self):
        self.retriever = RAGRetriever()
        self.response_generator = RAGResponseGenerator()
        self.context_assembler = ContextAssembler()
    
    def create_interface(self):
        """åˆ›å»ºRAGå¢å¼ºçš„èŠå¤©ç•Œé¢"""
        with gr.Blocks() as interface:
            gr.Markdown("# ğŸ¤– RAGSpace - AI Knowledge Hub")
            
            with gr.Row():
                with gr.Column(scale=3):
                    # DocSeté€‰æ‹©å™¨
                    docset_selector = gr.CheckboxGroup(
                        choices=self._get_docsets_list(),
                        label="é€‰æ‹©æ–‡æ¡£é›†åˆ",
                        value=["all"],
                        interactive=True
                    )
                    
                    # èŠå¤©å†å²
                    chatbot = gr.Chatbot(
                        height=400,
                        show_label=False
                    )
                    
                    # æŸ¥è¯¢è¾“å…¥
                    msg = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        lines=2
                    )
                    
                    with gr.Row():
                        submit = gr.Button("å‘é€", variant="primary")
                        clear = gr.Button("æ¸…é™¤")
                    
                    # æ£€ç´¢ç»“æœæ˜¾ç¤º
                    retrieval_results = gr.Markdown(
                        label="æ£€ç´¢ç»“æœ",
                        visible=False
                    )
                
                with gr.Column(scale=1):
                    # åµŒå…¥çŠ¶æ€
                    embedding_status = gr.Dataframe(
                        headers=["æ–‡æ¡£", "çŠ¶æ€", "æ›´æ–°æ—¶é—´"],
                        datatype=["str", "str", "str"],
                        col_count=(3, "fixed"),
                        interactive=False,
                        label="åµŒå…¥çŠ¶æ€"
                    )
                    
                    # åˆ·æ–°æŒ‰é’®
                    refresh_btn = gr.Button("åˆ·æ–°çŠ¶æ€")
                    
                    # æ‰‹åŠ¨è§¦å‘åµŒå…¥
                    trigger_embedding_btn = gr.Button("æ‰‹åŠ¨è§¦å‘åµŒå…¥")
            
            # äº‹ä»¶å¤„ç†
            submit.click(
                self._process_rag_query,
                inputs=[msg, chatbot, docset_selector],
                outputs=[chatbot, msg, retrieval_results]
            )
            
            clear.click(
                lambda: ([], "", gr.Markdown(visible=False)),
                outputs=[chatbot, msg, retrieval_results]
            )
            
            refresh_btn.click(
                self._update_embedding_status,
                outputs=embedding_status
            )
            
            trigger_embedding_btn.click(
                self._trigger_embedding_process,
                outputs=embedding_status
            )
        
        return interface
    
    def _process_rag_query(self, message: str, history: List[List[str]], 
                          docsets: List[str]) -> tuple:
        """å¤„ç†RAGæŸ¥è¯¢"""
        if not message.strip():
            return history, "", gr.Markdown(visible=False)
        
        try:
            # 1. æ£€ç´¢ç›¸å…³chunks
            chunks = self.retriever.hybrid_retrieve(message, docsets)
            
            # 2. ç»„è£…ä¸Šä¸‹æ–‡
            context = self.context_assembler.assemble_context(chunks)
            
            # 3. ç”Ÿæˆå“åº”
            response = self.response_generator.generate_response(message, context)
            
            # 4. æ„å»ºæ£€ç´¢ç»“æœæ˜¾ç¤º
            retrieval_display = self._build_retrieval_display(chunks)
            
            # 5. æ›´æ–°èŠå¤©å†å²
            history.append([message, response])
            
            return history, "", gr.Markdown(retrieval_display, visible=True)
            
        except Exception as e:
            error_response = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯: {str(e)}"
            history.append([message, error_response])
            return history, "", gr.Markdown(visible=False)
    
    def _build_retrieval_display(self, chunks: List[Dict]) -> str:
        """æ„å»ºæ£€ç´¢ç»“æœæ˜¾ç¤º"""
        if not chunks:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        display_parts = ["## ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£"]
        
        for i, chunk in enumerate(chunks, 1):
            metadata = chunk.get('metadata', {})
            
            # æ„å»ºæºä¿¡æ¯
            source_info = []
            if 'document_name' in metadata:
                source_info.append(f"**æ–‡æ¡£**: {metadata['document_name']}")
            if 'file_path' in metadata:
                source_info.append(f"**æ–‡ä»¶**: {metadata['file_path']}")
            if 'start_line' in metadata and 'end_line' in metadata:
                source_info.append(f"**è¡Œå·**: {metadata['start_line']}-{metadata['end_line']}")
            
            source_text = " | ".join(source_info) if source_info else "æœªçŸ¥æ¥æº"
            
            # æ„å»ºå†…å®¹é¢„è§ˆ
            content_preview = chunk['content'][:200] + "..." if len(chunk['content']) > 200 else chunk['content']
            
            display_parts.append(f"""
            ### ç»“æœ {i}
            
            {source_text}
            
            ```
            {content_preview}
            ```
            
            ---
            """)
        
        return "\n".join(display_parts)
    
    def _get_docsets_list(self) -> List[str]:
        """è·å–DocSetåˆ—è¡¨"""
        # ä»æ•°æ®åº“è·å–DocSetåˆ—è¡¨
        response = supabase.table("docsets").select("name").execute()
        return ["all"] + [docset["name"] for docset in response.data]
    
    def _update_embedding_status(self) -> List[List[str]]:
        """æ›´æ–°åµŒå…¥çŠ¶æ€"""
        response = supabase.table("documents").select(
            "title, embedding_status, embedding_updated_at"
        ).execute()
        
        status_data = []
        for doc in response.data:
            status_text = {
                'pending': 'ğŸŸ¡ ç­‰å¾…å¤„ç†',
                'processing': 'â³ å¤„ç†ä¸­...',
                'done': 'âœ… å·²å®Œæˆ',
                'error': 'âŒ å¤„ç†å¤±è´¥'
            }.get(doc['embedding_status'], 'â“ æœªçŸ¥çŠ¶æ€')
            
            status_data.append([
                doc['title'],
                status_text,
                doc.get('embedding_updated_at', 'N/A')
            ])
        
        return status_data
    
    def _trigger_embedding_process(self) -> List[List[str]]:
        """æ‰‹åŠ¨è§¦å‘åµŒå…¥å¤„ç†"""
        # è¿™é‡Œå¯ä»¥è°ƒç”¨åµŒå…¥å·¥ä½œå™¨
        # æš‚æ—¶è¿”å›æ›´æ–°åçš„çŠ¶æ€
        return self._update_embedding_status()
```

2. **é›†æˆåˆ°ä¸»åº”ç”¨**
```python
# src/ragspace/ui/components/__init__.py
from .rag_chat import RAGChatInterface

def create_rag_chat_tab():
    """åˆ›å»ºRAGèŠå¤©æ ‡ç­¾é¡µ"""
    rag_interface = RAGChatInterface()
    return rag_interface.create_interface()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] RAGèŠå¤©ç•Œé¢æ­£å¸¸å·¥ä½œ
- [ ] DocSeté€‰æ‹©å™¨åŠŸèƒ½æ­£ç¡®
- [ ] æ£€ç´¢ç»“æœæ˜¾ç¤ºæ¸…æ™°
- [ ] åµŒå…¥çŠ¶æ€æ˜¾ç¤ºå‡†ç¡®

### Phase 5: æµ‹è¯•å’Œä¼˜åŒ– (Week 5)

#### 5.1 å…¨é¢æµ‹è¯•

**ç›®æ ‡**: ç¡®ä¿RAGç³»ç»Ÿç¨³å®šå¯é 

**ä»»åŠ¡**:
- [ ] å•å…ƒæµ‹è¯•
- [ ] é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] ç”¨æˆ·éªŒæ”¶æµ‹è¯•

**æµ‹è¯•è®¡åˆ’**:

1. **å•å…ƒæµ‹è¯•**
```python
# tests/test_rag_system.py
import pytest
from src.ragspace.rag.text_splitter import RAGTextSplitter
from src.ragspace.rag.retriever import RAGRetriever
from src.ragspace.rag.response_generator import RAGResponseGenerator

class TestRAGSystem:
    def test_text_splitter(self):
        """æµ‹è¯•æ–‡æœ¬åˆ†ç‰‡å™¨"""
        splitter = RAGTextSplitter()
        
        # æµ‹è¯•æ–‡æœ¬åˆ†ç‰‡
        text = "This is a test document. It has multiple sentences. Each sentence should be properly split."
        chunks = splitter.split_text(text, "text")
        
        assert len(chunks) > 0
        assert all("content" in chunk for chunk in chunks)
        assert all("metadata" in chunk for chunk in chunks)
    
    def test_retriever(self):
        """æµ‹è¯•æ£€ç´¢å™¨"""
        retriever = RAGRetriever()
        
        # æµ‹è¯•å‘é‡æ£€ç´¢
        query = "How to use the API?"
        results = retriever.retrieve_chunks(query, top_k=3)
        
        assert isinstance(results, list)
        assert len(results) <= 3
    
    def test_response_generator(self):
        """æµ‹è¯•å“åº”ç”Ÿæˆå™¨"""
        generator = RAGResponseGenerator()
        
        # æµ‹è¯•å“åº”ç”Ÿæˆ
        query = "What is this about?"
        context = "This is a test context about API usage."
        
        response = generator.generate_response(query, context)
        
        assert isinstance(response, str)
        assert len(response) > 0
```

2. **é›†æˆæµ‹è¯•**
```python
# tests/test_rag_integration.py
class TestRAGIntegration:
    def test_end_to_end_rag_flow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯RAGæµç¨‹"""
        # 1. å‡†å¤‡æµ‹è¯•æ•°æ®
        test_document = {
            "title": "Test Document",
            "content": "This is a test document about API usage.",
            "docset_name": "test_docset"
        }
        
        # 2. æ·»åŠ æ–‡æ¡£åˆ°æ•°æ®åº“
        doc_id = self._add_test_document(test_document)
        
        # 3. è§¦å‘åµŒå…¥å¤„ç†
        self._trigger_embedding(doc_id)
        
        # 4. æ‰§è¡ŒæŸ¥è¯¢
        query = "How to use the API?"
        response = self._execute_rag_query(query, ["test_docset"])
        
        # 5. éªŒè¯ç»“æœ
        assert response is not None
        assert len(response) > 0
```

3. **æ€§èƒ½æµ‹è¯•**
```python
# tests/test_rag_performance.py
import time

class TestRAGPerformance:
    def test_retrieval_performance(self):
        """æµ‹è¯•æ£€ç´¢æ€§èƒ½"""
        retriever = RAGRetriever()
        
        start_time = time.time()
        results = retriever.retrieve_chunks("test query", top_k=5)
        end_time = time.time()
        
        # æ£€ç´¢æ—¶é—´åº”è¯¥åœ¨1ç§’ä»¥å†…
        assert end_time - start_time < 1.0
        assert len(results) <= 5
    
    def test_response_generation_performance(self):
        """æµ‹è¯•å“åº”ç”Ÿæˆæ€§èƒ½"""
        generator = RAGResponseGenerator()
        
        start_time = time.time()
        response = generator.generate_response("test query", "test context")
        end_time = time.time()
        
        # å“åº”ç”Ÿæˆæ—¶é—´åº”è¯¥åœ¨5ç§’ä»¥å†…
        assert end_time - start_time < 5.0
        assert len(response) > 0
```

#### 5.2 æ€§èƒ½ä¼˜åŒ–

**ç›®æ ‡**: ä¼˜åŒ–RAGç³»ç»Ÿæ€§èƒ½

**ä»»åŠ¡**:
- [ ] æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
- [ ] ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
- [ ] å¹¶å‘å¤„ç†ä¼˜åŒ–
- [ ] å†…å­˜ä½¿ç”¨ä¼˜åŒ–

**ä¼˜åŒ–ç­–ç•¥**:

1. **æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–**
```sql
-- ä¼˜åŒ–å‘é‡ç´¢å¼•
CREATE INDEX CONCURRENTLY ON chunks USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- æ·»åŠ å¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY idx_chunks_docset_status ON chunks(docset_name, embedding_status);

-- æ·»åŠ éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•å·²åµŒå…¥çš„chunksï¼‰
CREATE INDEX CONCURRENTLY idx_chunks_embedded ON chunks(embedding) 
WHERE embedding IS NOT NULL;
```

2. **ç¼“å­˜ç­–ç•¥ä¼˜åŒ–**
```python
# å¤šçº§ç¼“å­˜ç­–ç•¥
class MultiLevelCache:
    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜
        self.l2_cache = redis.Redis()  # Redisç¼“å­˜
    
    def get(self, key: str):
        # L1ç¼“å­˜æŸ¥æ‰¾
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2ç¼“å­˜æŸ¥æ‰¾
        value = self.l2_cache.get(key)
        if value:
            # æ›´æ–°L1ç¼“å­˜
            self.l1_cache[key] = value
            return value
        
        return None
    
    def set(self, key: str, value: str, ttl: int = 300):
        # è®¾ç½®L1ç¼“å­˜
        self.l1_cache[key] = value
        
        # è®¾ç½®L2ç¼“å­˜
        self.l2_cache.setex(key, ttl, value)
```

3. **å¹¶å‘å¤„ç†ä¼˜åŒ–**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncRAGProcessor:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def process_multiple_queries(self, queries: List[str]) -> List[str]:
        """å¹¶å‘å¤„ç†å¤šä¸ªæŸ¥è¯¢"""
        loop = asyncio.get_event_loop()
        
        # å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢
        tasks = [
            loop.run_in_executor(self.executor, self._process_single_query, query)
            for query in queries
        ]
        
        results = await asyncio.gather(*tasks)
        return results
    
    def _process_single_query(self, query: str) -> str:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        retriever = RAGRetriever()
        response_generator = RAGResponseGenerator()
        
        chunks = retriever.retrieve_chunks(query)
        context = ContextAssembler().assemble_context(chunks)
        response = response_generator.generate_response(query, context)
        
        return response
```

## éªŒæ”¶æ ‡å‡†

### Phase 1 éªŒæ”¶æ ‡å‡†
- [ ] chunksè¡¨æˆåŠŸåˆ›å»ºå¹¶æ­£å¸¸å·¥ä½œ
- [ ] æ–‡æœ¬åˆ†ç‰‡å™¨æ­£ç¡®å¤„ç†å„ç§æ–‡æ¡£ç±»å‹
- [ ] åµŒå…¥å·¥ä½œå™¨èƒ½å¤Ÿå¼‚æ­¥å¤„ç†æ–‡æ¡£
- [ ] çŠ¶æ€ç®¡ç†ç³»ç»Ÿæ­£ç¡®è·Ÿè¸ªå¤„ç†çŠ¶æ€

### Phase 2 éªŒæ”¶æ ‡å‡†
- [ ] å‘é‡æ£€ç´¢åŠŸèƒ½æ­£å¸¸ï¼Œå“åº”æ—¶é—´ < 1ç§’
- [ ] GPTé‡æ’åºåŠŸèƒ½æ­£ç¡®å·¥ä½œ
- [ ] æ··åˆæ£€ç´¢ç­–ç•¥æœ‰æ•ˆ
- [ ] ç¼“å­˜æœºåˆ¶æ˜¾è‘—æå‡æ€§èƒ½

### Phase 3 éªŒæ”¶æ ‡å‡†
- [ ] LLMå“åº”ç”ŸæˆåŠŸèƒ½æ­£å¸¸
- [ ] æµå¼å“åº”æ”¯æŒè‰¯å¥½
- [ ] å¯¹è¯å†å²ç®¡ç†æ­£ç¡®
- [ ] å“åº”è´¨é‡è¯„ä¼°å‡†ç¡®

### Phase 4 éªŒæ”¶æ ‡å‡†
- [ ] RAGèŠå¤©ç•Œé¢é›†æˆæˆåŠŸ
- [ ] DocSeté€‰æ‹©å™¨åŠŸèƒ½æ­£ç¡®
- [ ] æ£€ç´¢ç»“æœæ˜¾ç¤ºæ¸…æ™°
- [ ] åµŒå…¥çŠ¶æ€æ˜¾ç¤ºå‡†ç¡®

### Phase 5 éªŒæ”¶æ ‡å‡†
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°è¦æ±‚
- [ ] ç³»ç»Ÿç¨³å®šå¯é 
- [ ] ç”¨æˆ·ä½“éªŒè‰¯å¥½

## é£é™©è¯„ä¼°å’Œç¼“è§£ç­–ç•¥

### æŠ€æœ¯é£é™©

1. **OpenAI APIé™åˆ¶**
   - **é£é™©**: APIè°ƒç”¨é™åˆ¶å’Œæˆæœ¬æ§åˆ¶
   - **ç¼“è§£**: å®ç°ç¼“å­˜æœºåˆ¶ï¼Œæ·»åŠ æˆæœ¬ç›‘æ§

2. **å‘é‡æ•°æ®åº“æ€§èƒ½**
   - **é£é™©**: å¤§è§„æ¨¡æ•°æ®ä¸‹çš„æŸ¥è¯¢æ€§èƒ½
   - **ç¼“è§£**: ä¼˜åŒ–ç´¢å¼•ï¼Œå®ç°åˆ†é¡µæŸ¥è¯¢

3. **å¹¶å‘å¤„ç†å¤æ‚æ€§**
   - **é£é™©**: å¼‚æ­¥å¤„ç†çš„å¤æ‚æ€§
   - **ç¼“è§£**: ä½¿ç”¨æˆç†Ÿçš„å¼‚æ­¥æ¡†æ¶ï¼Œå……åˆ†æµ‹è¯•

### ä¸šåŠ¡é£é™©

1. **ç”¨æˆ·ä½“éªŒ**
   - **é£é™©**: RAGç³»ç»Ÿå“åº”æ—¶é—´è¿‡é•¿
   - **ç¼“è§£**: å®ç°æµå¼å“åº”ï¼Œä¼˜åŒ–ç¼“å­˜ç­–ç•¥

2. **æ•°æ®è´¨é‡**
   - **é£é™©**: æ£€ç´¢ç»“æœä¸å‡†ç¡®
   - **ç¼“è§£**: å®ç°å¤šçº§æ£€ç´¢ç­–ç•¥ï¼Œæ·»åŠ è´¨é‡è¯„ä¼°

## æ€»ç»“

æœ¬RAGå®ç°è®¡åˆ’æä¾›äº†è¯¦ç»†çš„å®æ–½æ­¥éª¤å’ŒæŠ€æœ¯æ–¹æ¡ˆï¼Œç¡®ä¿RAGSpaceé¡¹ç›®èƒ½å¤ŸæˆåŠŸå®ç°é«˜è´¨é‡çš„æ–‡æ¡£æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½ã€‚é€šè¿‡åˆ†é˜¶æ®µå®æ–½å’Œä¸¥æ ¼çš„éªŒæ”¶æ ‡å‡†ï¼Œå¯ä»¥ç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚

---

*å®ç°è®¡åˆ’ç‰ˆæœ¬: 1.0*  
*æ›´æ–°æ—¶é—´: 2025-08-07*
